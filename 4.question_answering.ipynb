{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai==0.27.8\n",
    "# !pip install langchain==0.0.212\n",
    "# !pip install chromadb==0.3.26\n",
    "# !pip install jupyterlab\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install tiktoken==0.4.0\n",
    "# !pip install evaluate==0.4.0\n",
    "# !pip install nltk==3.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- Description\n",
    "- SQuAD\n",
    "- Using LLM for Q&A\n",
    "- Adding context\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to explore Qustion Answering using ChatGPT. \n",
    "We will consider two ways:\n",
    "\n",
    "1. Q&A without context\n",
    "\n",
    "2. Contextual retrieval Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question answering in this context refers to question answering over your document data. We are going to predict answers for the questions in SQuAD dataset using LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford Question Answering Dataset [(SQuAD)](https://huggingface.co/datasets/squad) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use \"text-davinci-003\" model from OpenAI. It is large language model from GPT-3.5 family. They can understand and generate natural language or code. \"text-davinci-003\" can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness. We will use OpenAI Embeddings API for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convinient usage of embeddings with dataset we are going to use vector database.\n",
    "\n",
    "A vector database is a kind of database specifically designed to handle vector data. Vector data is a type of data that can be represented by a set of numerical values in a multidimensional space. In the context of machine learning and AI, vector data often refers to embeddings, which are high-dimensional vectors that encode information about data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice, Q&A task incudes few components (data, llm, embeddings) that needs to be used together for getting ready to use solution.\n",
    "\n",
    "[LangChain](https://js.langchain.com/docs/) provide very convinient framework to combine these components together.\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. It enables applications that are:\n",
    "\n",
    "- Data-aware: connect a language model to other sources of data\n",
    "- Agentic: allow a language model to interact with its environment\n",
    "\n",
    "The main value props of LangChain are:\n",
    "\n",
    "- Components: abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\n",
    "- Off-the-shelf chains: a structured assembly of components for accomplishing specific higher-level tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SQuAD dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o data/raw_train.json -X GET \\\n",
    "    \"https://datasets-server.huggingface.co/rows?dataset=squad&config=plain_text&split=train&offset=0&limit=100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets convert json data to csv format for more convinient usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_squad_to_csv(json_file, csv_file):\n",
    "    \"\"\"Convert raw SQuAD json data to csv format\"\"\"\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    id_list = []\n",
    "    context_list = []\n",
    "    question_list = []\n",
    "    answer_list = []\n",
    "\n",
    "    for row in squad_dict['rows']:\n",
    "        data = row['row']\n",
    "        id_list.append(data['id'])\n",
    "        context_list.append(data['context'])\n",
    "        question_list.append(data['question'])\n",
    "        answer_list.append(data['answers']['text'][0])\n",
    "\n",
    "    df = pd.DataFrame(zip(id_list, question_list, context_list, answer_list), \n",
    "                      columns=['id', 'question', 'context', 'answer'])\n",
    "    df.to_csv(csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_squad_to_csv('data/raw_train.json', 'data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how is our data distribute, we are most interested in columns 'question' and 'context'. We want to know the \"size\" of our data in order to be able to estimate costs for API usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  5733be284776f41900661182   \n",
       "1  5733be284776f4190066117f   \n",
       "2  5733be284776f41900661180   \n",
       "3  5733be284776f41900661181   \n",
       "4  5733be284776f4190066117e   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                    answer  \n",
       "0               Saint Bernadette Soubirous  \n",
       "1                a copper statue of Christ  \n",
       "2                        the Main Building  \n",
       "3  a Marian place of prayer and reflection  \n",
       "4       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "print(len(train_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['question'].map(len).hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['context'].drop_duplicates().map(len).hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform Q&A with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot learning is a machine learning approach where the model learns to make accurate predictions from a very small number of examples, typically in the order of one to five. This technique offers significant benefits such as efficiency in handling sparse data and the ability to generalize well from limited information, making it a versatile choice for tasks where acquiring large labeled datasets is challenging or costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(df):\n",
    "    examples = []\n",
    "    for i, row in df.sample(3).iterrows():\n",
    "        example = {\n",
    "            \"query\": row['question'],\n",
    "            \"answer\": row['answer']\n",
    "        }\n",
    "        examples.append(example)\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect examples from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'How many stories tall is the main library at Notre Dame?',\n",
       "  'answer': '14'},\n",
       " {'query': 'Which prize did Frederick Buechner create?',\n",
       "  'answer': 'Buechner Prize for Preaching'},\n",
       " {'query': 'There are how many dorms for females at Notre Dame?',\n",
       "  'answer': '14'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = get_examples(train_df)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"You are a useful assistant with artificial intelligence who gives \n",
    "short but accurate answers to the questions asked.\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our few-shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a useful assistant with artificial intelligence who gives \n",
      "short but accurate answers to the questions asked.\n",
      "examples: \n",
      "\n",
      "\n",
      "\n",
      "User: How many stories tall is the main library at Notre Dame?\n",
      "AI: 14\n",
      "\n",
      "\n",
      "\n",
      "User: Which prize did Frederick Buechner create?\n",
      "AI: Buechner Prize for Preaching\n",
      "\n",
      "\n",
      "\n",
      "User: There are how many dorms for females at Notre Dame?\n",
      "AI: 14\n",
      "\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(few_shot_prompt_template.format(query=\"What is the meaning of life?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets evaluate our model on 30 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = train_df.sample(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Use LLM to predict answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "# initialize the models\n",
    "llm = OpenAI(\n",
    "    model_name=\"text-davinci-003\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [LLMchain](https://js.langchain.com/docs/modules/chains/llm_chain) to wrap our prompt template with a llm. An LLMChain is a simple chain that adds some functionality around language models, in our case we put llm and prompt template together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict answer for each question, we will keep answers from our llm in \"llm_answers\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['llm_answers'] = samples['question'].map(llm.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_csv('data/sample_answers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see at sample of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>llm_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5733b5344776f419006610e0</td>\n",
       "      <td>In what year did Notre Dame begin to host the ...</td>\n",
       "      <td>As of 2012[update] research continued in many ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>5733b2fe4776f41900661093</td>\n",
       "      <td>The Lobund Institute was merged into the Depar...</td>\n",
       "      <td>The Lobund Institute grew out of pioneering re...</td>\n",
       "      <td>1958</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5733a70c4776f41900660f64</td>\n",
       "      <td>What entity provides help with the management ...</td>\n",
       "      <td>All of Notre Dame's undergraduate students are...</td>\n",
       "      <td>Learning Resource Center</td>\n",
       "      <td>The First Year of Studies Office.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5733a6424776f41900660f4f</td>\n",
       "      <td>Before the creation of the College of Engineer...</td>\n",
       "      <td>The College of Engineering was established in ...</td>\n",
       "      <td>the College of Science</td>\n",
       "      <td>College of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5733bed24776f41900661188</td>\n",
       "      <td>Where is the headquarters of the Congregation ...</td>\n",
       "      <td>The university is the major seat of the Congre...</td>\n",
       "      <td>Rome</td>\n",
       "      <td>Rome, Italy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "71  5733b5344776f419006610e0   \n",
       "62  5733b2fe4776f41900661093   \n",
       "20  5733a70c4776f41900660f64   \n",
       "17  5733a6424776f41900660f4f   \n",
       "10  5733bed24776f41900661188   \n",
       "\n",
       "                                             question  \\\n",
       "71  In what year did Notre Dame begin to host the ...   \n",
       "62  The Lobund Institute was merged into the Depar...   \n",
       "20  What entity provides help with the management ...   \n",
       "17  Before the creation of the College of Engineer...   \n",
       "10  Where is the headquarters of the Congregation ...   \n",
       "\n",
       "                                              context  \\\n",
       "71  As of 2012[update] research continued in many ...   \n",
       "62  The Lobund Institute grew out of pioneering re...   \n",
       "20  All of Notre Dame's undergraduate students are...   \n",
       "17  The College of Engineering was established in ...   \n",
       "10  The university is the major seat of the Congre...   \n",
       "\n",
       "                      answer                         llm_answers  \n",
       "71                      2013                                2016  \n",
       "62                      1958                                1966  \n",
       "20  Learning Resource Center   The First Year of Studies Office.  \n",
       "17    the College of Science                  College of Science  \n",
       "10                      Rome                         Rome, Italy  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model can stick to the correct format, but the facts can be distorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Evaluating LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per we have ground truths, we can measure the LLM quality on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use squad and ROUGE metric to evaluate Q&A score.\n",
    "\n",
    "[squad score](https://huggingface.co/spaces/evaluate-metric/squad_v2) - wraps the official scoring script for evaluating in SQuAD chellenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`exact`: Exact match (the normalized answer exactly match the gold answer) (see the exact_match metric (forthcoming))\\\n",
    "`f1`: The average F1-score of predicted tokens versus the gold answer (see the F1 score metric)\\\n",
    "`total`: Number of scores considered'\\\n",
    "`HasAns_exact`: Exact match (the normalized answer exactly match the gold answer)\\\n",
    "`HasAns_f1`: The F-score of predicted tokens versus the gold answer\\\n",
    "`HasAns_total`: How many of the questions have answers\\\n",
    "`NoAns_exact`: Exact match (the normalized answer exactly match the gold answer)\\\n",
    "`NoAns_f1`: The F-score of predicted tokens versus the gold answer\\\n",
    "`NoAns_total`: How many of the questions have no answers\\\n",
    "`best_exact` : Best exact match (with varying threshold)\\\n",
    "`best_exact_thresh`: No-answer probability threshold associated to the best exact match\\\n",
    "`best_f1`: Best F1 score (with varying threshold)\\\n",
    "`best_f1_thresh`: No-answer probability threshold associated to the best F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)), or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics used for evaluating automatic summarization and machine translation. ROUGE includes measures such as:\n",
    "\n",
    "- ROUGE-N: Overlap of N-grams between the system and reference summaries.\n",
    "- ROUGE-L: Longest Common Subsequence (LCS) based statistics. Longest common subsequence problem takes into account sentence level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Answering (QA) tasks often require a system to generate precise and concise answers to specific questions. In some QA scenarios, especially those involving more complex or open-ended questions, the answers can be quite long, and there can be more than one correct answer. In such cases, using a metric like Exact Match (EM), which requires the system's output to match the reference answer exactly, might not be appropriate.\n",
    "\n",
    "ROUGE metrics, initially developed for evaluating automatic summarization tasks, measure the overlap of n-grams between the system output and the reference text. ROUGE can be more flexible and forgiving than EM because it doesn't require the system's output to exactly match the reference. Instead, it measures the extent to which the key pieces of information (as represented by the n-grams) in the reference text also appear in the system's output.\n",
    "\n",
    "So, if you have a QA task where the answers and reference texts are quite long, and you're more interested in the system's ability to include all the key pieces of information in its answer rather than its ability to exactly reproduce the reference text, then ROUGE could be an appropriate choice of metric.\n",
    "\n",
    "For instance, ROUGE-1 or ROUGE-2 could be used to measure the overlap of unigrams or bigrams, respectively, which can give a sense of whether the system is including all the key terms or pairs of terms. ROUGE-L, which measures the longest common subsequence, could be used to measure whether the system is maintaining the same general order of information as in the reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "squad_metric = evaluate.load(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm(samples, pred_col='llm_answers'):\n",
    "    references = samples['answer'].values\n",
    "    predictions = samples[pred_col].values\n",
    "\n",
    "    rouge_res = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "    references, predictions = [], []\n",
    "\n",
    "    for _, row in samples.iterrows():\n",
    "        predict = {\n",
    "            'prediction_text': row[pred_col],\n",
    "            'id': row['id'],\n",
    "            'no_answer_probability': 0.\n",
    "        }\n",
    "        predictions.append(predict)\n",
    "        \n",
    "        reference = {\n",
    "            'answers': {\n",
    "                'text': [row['answer']],\n",
    "                'answer_start': [0]\n",
    "            },\n",
    "            'id': row['id']\n",
    "        }\n",
    "        references.append(reference)\n",
    "\n",
    "    res = squad_metric.compute(predictions=predictions, references=references)\n",
    "    res.update(rouge_res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 10.0,\n",
       " 'f1': 28.064213564213567,\n",
       " 'total': 30,\n",
       " 'HasAns_exact': 10.0,\n",
       " 'HasAns_f1': 28.064213564213567,\n",
       " 'HasAns_total': 30,\n",
       " 'best_exact': 10.0,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 28.064213564213567,\n",
       " 'best_f1_thresh': 0.0,\n",
       " 'rouge1': 0.2881613756613757,\n",
       " 'rouge2': 0.11999999999999998,\n",
       " 'rougeL': 0.2894708994708995,\n",
       " 'rougeLsum': 0.2831283068783069}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_llm(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rouge score is not well, lets try to improve it by using context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide context to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data retrivial](https://python.langchain.com/assets/images/data_connection-c42d68c3d092b85f50d08d4cc171fc25.jpg)\n",
    "https://python.langchain.com/docs/modules/data_connection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite OpenAI text-davinci-003 was trained on huge amount of data including wikipedia and SQuAd, model can forget some information due to parameters size and data size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a llm can continue sequence by given sequence, we can engage context for more robust and accurate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For question answering over many documents, you almost always want to create an index over the data. This can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to provide context along the answers to our llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(train_df.drop_duplicates(subset=['context'])[['context']], page_content_column=\"context\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', metadata={}),\n",
       " Document(page_content=\"As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.\", metadata={})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Store context and its embeddings in Chroma vector db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma is the open-source embedding database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Croma](https://docs.trychroma.com/img/hrm4.svg)\n",
    "https://docs.trychroma.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma gives you the tools to:\n",
    "\n",
    "- store embeddings and their metadata\n",
    "- embed documents and queries\n",
    "- search embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use OpenAI Embeddigs API to get vector representation of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store text along with embeddings. Under the hood, we are saying OpenAI API: generate vector representation for each document we provided. After that we put these vectors along with this documents itself.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create our vector store.\n",
    "\n",
    "We pass `data` and `embeddings` as an arguments. \n",
    "\n",
    "In real-life applications, we want to store our vectors and index in a persistent manner. To achieve this, we are going to pass the `persist_directory` argument, which specifies where to store our data. Then, `db.persist()` will persist the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "PERSIST_DIRECTORY = 'data/vstore'\n",
    "\n",
    "db = Chroma.from_documents(data, embeddings, persist_directory=PERSIST_DIRECTORY)\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use index to get context which would be most appropriate in terms of similarity to query we will provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets show how it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    \"\"\"Helper function for printing docs\"\"\"\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "The library system of the university is divided between the main library and each of the colleges and schools. The main building is the 14-story Theodore M. Hesburgh Library, completed in 1963, which is the third building to house the main collection of books. The front of the library is adorned with the Word of Life mural designed by artist Millard Sheets. This mural is popularly known as \"Touchdown Jesus\" because of its proximity to Notre Dame Stadium and Jesus' arms appearing to make the signal for a touchdown.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "The university is affiliated with the Congregation of Holy Cross (Latin: Congregatio a Sancta Cruce, abbreviated postnominals: \"CSC\"). While religious affiliation is not a criterion for admission, more than 93% of students identify as Christian, with over 80% of the total being Catholic. Collectively, Catholic Mass is celebrated over 100 times per week on campus, and a large campus ministry program provides for the faith needs of the community. There are multitudes of religious statues and artwork around campus, most prominent of which are the statue of Mary on the Main Building, the Notre Dame Grotto, and the Word of Life mural on Hesburgh Library depicting Christ as a teacher. Additionally, every classroom displays a crucifix. There are many religious clubs (catholic and non-Catholic) at the school, including Council #1477 of the Knights of Columbus (KOC), Baptist Collegiate Ministry (BCM), Jewish Club, Muslim Student Association, Orthodox Christian Fellowship, The Mormon Club, and many more. The Notre Dame KofC are known for being the first collegiate council of KofC, operating a charitable concession stand during every home football game and owning their own building on campus which can be used as a cigar lounge. Fifty-seven chapels are located throughout the campus.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "As of 2012[update] research continued in many fields. The university president, John Jenkins, described his hope that Notre Dame would become \"one of the pre–eminent research institutions in the world\" in his inaugural address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet. As of 2013, the university is home to the Notre Dame Global Adaptation Index which ranks countries annually based on how vulnerable they are to climate change and how prepared they are to adapt.\n"
     ]
    }
   ],
   "source": [
    "matches = retriever.vectorstore.search(\"What is in front of the Notre Dame Main Building?\", 'similarity')\n",
    "pretty_print_docs(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a query and expect to get context which is most similar to our query. How do we decide which context is more similar than another one? That's why we use embeddings. Embeddings can represent our text in a way that keeps similar texts close in vector space and dissimilar ones far away. When we use a retriever, we are comparing the query embedding with each context embedding and picking up the closest one. In cell above we have picked up five most similar context sorted by relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if you want to load vector db from disk, you have to use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Retrieving context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://python.langchain.com/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg)\n",
    "https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains in LangChain is just a wappers for common components like llms, prompts, etc. We are going to use built in chain [qa_chain](https://python.langchain.com/docs/modules/chains/additional/question_answering.html) specializing for QA tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new prompt, this time taking into account the additional context we retrieve from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"You are a useful assistant with artificial intelligence who gives \n",
    "short but accurate answers to the questions asked using provided context.\n",
    "examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "context: {context}\n",
    "\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For qa chain api we need to explicitly provide query with documents, we can make it by using retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is in front of the Notre Dame Main Building?'\n",
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at our chain:\n",
    "\n",
    "1. We use `chain_type=\"stuff\"` - it means that we take a list of documents, inserts them all into a prompt and passes that prompt to an LLM.\n",
    "2. `model='text-davinci-003'` - we use Completion API using model OpenAI text-davinci-003\n",
    "3. `temperature=0` - set temperature parameter for sampling\n",
    "4. `\"input_documents\"` - this parameter is in charge of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a useful assistant with artificial intelligence who gives \n",
      "short but accurate answers to the questions asked using provided context.\n",
      "examples: \n",
      "\n",
      "\n",
      "\n",
      "User: How many stories tall is the main library at Notre Dame?\n",
      "AI: 14\n",
      "\n",
      "\n",
      "\n",
      "User: Which prize did Frederick Buechner create?\n",
      "AI: Buechner Prize for Preaching\n",
      "\n",
      "\n",
      "\n",
      "User: There are how many dorms for females at Notre Dame?\n",
      "AI: 14\n",
      "\n",
      "\n",
      "\n",
      "context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\n",
      "The library system of the university is divided between the main library and each of the colleges and schools. The main building is the 14-story Theodore M. Hesburgh Library, completed in 1963, which is the third building to house the main collection of books. The front of the library is adorned with the Word of Life mural designed by artist Millard Sheets. This mural is popularly known as \"Touchdown Jesus\" because of its proximity to Notre Dame Stadium and Jesus' arms appearing to make the signal for a touchdown.\n",
      "\n",
      "The university is affiliated with the Congregation of Holy Cross (Latin: Congregatio a Sancta Cruce, abbreviated postnominals: \"CSC\"). While religious affiliation is not a criterion for admission, more than 93% of students identify as Christian, with over 80% of the total being Catholic. Collectively, Catholic Mass is celebrated over 100 times per week on campus, and a large campus ministry program provides for the faith needs of the community. There are multitudes of religious statues and artwork around campus, most prominent of which are the statue of Mary on the Main Building, the Notre Dame Grotto, and the Word of Life mural on Hesburgh Library depicting Christ as a teacher. Additionally, every classroom displays a crucifix. There are many religious clubs (catholic and non-Catholic) at the school, including Council #1477 of the Knights of Columbus (KOC), Baptist Collegiate Ministry (BCM), Jewish Club, Muslim Student Association, Orthodox Christian Fellowship, The Mormon Club, and many more. The Notre Dame KofC are known for being the first collegiate council of KofC, operating a charitable concession stand during every home football game and owning their own building on campus which can be used as a cigar lounge. Fifty-seven chapels are located throughout the campus.\n",
      "\n",
      "As of 2012[update] research continued in many fields. The university president, John Jenkins, described his hope that Notre Dame would become \"one of the pre–eminent research institutions in the world\" in his inaugural address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet. As of 2013, the university is home to the Notre Dame Global Adaptation Index which ranks countries annually based on how vulnerable they are to climate change and how prepared they are to adapt.\n",
      "\n",
      "User: What is in front of the Notre Dame Main Building?\n",
      "AI: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output_text': ' A copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\".'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_chain(OpenAI(model='text-davinci-003', temperature=0), chain_type=\"stuff\", prompt=few_shot_prompt_template, verbose=True)\n",
    "chain({\"input_documents\": docs, \"query\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can explain what happens under the hood:\n",
    "\n",
    "1. We take out prompt template with examples how our answer should looks like\n",
    "2. We select 5 most similar contexts to our query and insert it inside our prompt along with query (final prompt marked as green text)\n",
    "3. Provide it to the LLM\n",
    "4. Collect the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see prompt and another debug information we use `verbose=True` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Make predictions with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_with_context(query, retriever=retriever, prompt=few_shot_prompt_template):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    chain = load_qa_chain(OpenAI(model='text-davinci-003', temperature=0), chain_type=\"stuff\", prompt=prompt)\n",
    "    out = chain({\"input_documents\": docs, \"query\": query}, return_only_outputs=True)\n",
    "    return out['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['llm_answers_with_context'] = samples['question'].map(make_prediction_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_csv('data/sample_answers_with_context.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = pd.read_csv('data/sample_answers_with_context.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>llm_answers</th>\n",
       "      <th>llm_answers_with_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5733b2fe4776f41900661092</td>\n",
       "      <td>In what year did Lobund at Notre Dame become a...</td>\n",
       "      <td>The Lobund Institute grew out of pioneering re...</td>\n",
       "      <td>1950</td>\n",
       "      <td>1949</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5733b2fe4776f41900661093</td>\n",
       "      <td>The Lobund Institute was merged into the Depar...</td>\n",
       "      <td>The Lobund Institute grew out of pioneering re...</td>\n",
       "      <td>1958</td>\n",
       "      <td>1966</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>57338724d058e614000b5c9d</td>\n",
       "      <td>In 1919 a new president of Notre Dame was name...</td>\n",
       "      <td>In 1919 Father James Burns became president of...</td>\n",
       "      <td>Father James Burns</td>\n",
       "      <td>Edward Frederick O'Hara.</td>\n",
       "      <td>Father James Burns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733a7bd4776f41900660f6d</td>\n",
       "      <td>Which department at Notre Dame is the only one...</td>\n",
       "      <td>The university first offered graduate degrees,...</td>\n",
       "      <td>Department of Pre-Professional Studies</td>\n",
       "      <td>Mendoza College of Business</td>\n",
       "      <td>Pre-Professional Studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5733b0fb4776f41900661042</td>\n",
       "      <td>What professorship did Father Josh Carrier hol...</td>\n",
       "      <td>Father Joseph Carrier, C.S.C. was Director of ...</td>\n",
       "      <td>Professor of Chemistry and Physics</td>\n",
       "      <td>Charles E. Sheedy, CSC, Professorship in the ...</td>\n",
       "      <td>Professor of Chemistry and Physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5733a6424776f41900660f51</td>\n",
       "      <td>How many BS level degrees are offered in the C...</td>\n",
       "      <td>The College of Engineering was established in ...</td>\n",
       "      <td>eight</td>\n",
       "      <td>eight</td>\n",
       "      <td>Eight B.S. degrees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>57338724d058e614000b5c9e</td>\n",
       "      <td>Over how many years did the change to national...</td>\n",
       "      <td>In 1919 Father James Burns became president of...</td>\n",
       "      <td>three years</td>\n",
       "      <td>10 years</td>\n",
       "      <td>Three years.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5733bed24776f41900661188</td>\n",
       "      <td>Where is the headquarters of the Congregation ...</td>\n",
       "      <td>The university is the major seat of the Congre...</td>\n",
       "      <td>Rome</td>\n",
       "      <td>Rome, Italy</td>\n",
       "      <td>Rome, Italy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>The Grotto of Our Lady of Lourdes.</td>\n",
       "      <td>The Main Building.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5733b5344776f419006610de</td>\n",
       "      <td>The Kellogg Institute for International Studie...</td>\n",
       "      <td>As of 2012[update] research continued in many ...</td>\n",
       "      <td>Notre Dame</td>\n",
       "      <td>University of Notre Dame</td>\n",
       "      <td>University of Notre Dame</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "21  5733b2fe4776f41900661092   \n",
       "16  5733b2fe4776f41900661093   \n",
       "26  57338724d058e614000b5c9d   \n",
       "0   5733a7bd4776f41900660f6d   \n",
       "15  5733b0fb4776f41900661042   \n",
       "8   5733a6424776f41900660f51   \n",
       "23  57338724d058e614000b5c9e   \n",
       "29  5733bed24776f41900661188   \n",
       "9   5733be284776f41900661180   \n",
       "11  5733b5344776f419006610de   \n",
       "\n",
       "                                             question  \\\n",
       "21  In what year did Lobund at Notre Dame become a...   \n",
       "16  The Lobund Institute was merged into the Depar...   \n",
       "26  In 1919 a new president of Notre Dame was name...   \n",
       "0   Which department at Notre Dame is the only one...   \n",
       "15  What professorship did Father Josh Carrier hol...   \n",
       "8   How many BS level degrees are offered in the C...   \n",
       "23  Over how many years did the change to national...   \n",
       "29  Where is the headquarters of the Congregation ...   \n",
       "9   The Basilica of the Sacred heart at Notre Dame...   \n",
       "11  The Kellogg Institute for International Studie...   \n",
       "\n",
       "                                              context  \\\n",
       "21  The Lobund Institute grew out of pioneering re...   \n",
       "16  The Lobund Institute grew out of pioneering re...   \n",
       "26  In 1919 Father James Burns became president of...   \n",
       "0   The university first offered graduate degrees,...   \n",
       "15  Father Joseph Carrier, C.S.C. was Director of ...   \n",
       "8   The College of Engineering was established in ...   \n",
       "23  In 1919 Father James Burns became president of...   \n",
       "29  The university is the major seat of the Congre...   \n",
       "9   Architecturally, the school has a Catholic cha...   \n",
       "11  As of 2012[update] research continued in many ...   \n",
       "\n",
       "                                    answer  \\\n",
       "21                                    1950   \n",
       "16                                    1958   \n",
       "26                      Father James Burns   \n",
       "0   Department of Pre-Professional Studies   \n",
       "15      Professor of Chemistry and Physics   \n",
       "8                                    eight   \n",
       "23                             three years   \n",
       "29                                    Rome   \n",
       "9                        the Main Building   \n",
       "11                              Notre Dame   \n",
       "\n",
       "                                          llm_answers  \\\n",
       "21                                               1949   \n",
       "16                                               1966   \n",
       "26                           Edward Frederick O'Hara.   \n",
       "0                         Mendoza College of Business   \n",
       "15   Charles E. Sheedy, CSC, Professorship in the ...   \n",
       "8                                               eight   \n",
       "23                                           10 years   \n",
       "29                                        Rome, Italy   \n",
       "9                  The Grotto of Our Lady of Lourdes.   \n",
       "11                           University of Notre Dame   \n",
       "\n",
       "               llm_answers_with_context  \n",
       "21                                 1950  \n",
       "16                                 1958  \n",
       "26                   Father James Burns  \n",
       "0              Pre-Professional Studies  \n",
       "15   Professor of Chemistry and Physics  \n",
       "8                   Eight B.S. degrees.  \n",
       "23                         Three years.  \n",
       "29                         Rome, Italy.  \n",
       "9                    The Main Building.  \n",
       "11             University of Notre Dame  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets measure the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 10.0,\n",
       " 'f1': 28.064213564213567,\n",
       " 'total': 30,\n",
       " 'HasAns_exact': 10.0,\n",
       " 'HasAns_f1': 28.064213564213567,\n",
       " 'HasAns_total': 30,\n",
       " 'best_exact': 10.0,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 28.064213564213567,\n",
       " 'best_f1_thresh': 0.0,\n",
       " 'rouge1': 0.28449735449735447,\n",
       " 'rouge2': 0.11888888888888888,\n",
       " 'rougeL': 0.28488756613756616,\n",
       " 'rougeLsum': 0.2876917989417989}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_llm(samples, pred_col='llm_answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 66.66666666666667,\n",
       " 'f1': 77.90196078431373,\n",
       " 'total': 30,\n",
       " 'HasAns_exact': 66.66666666666667,\n",
       " 'HasAns_f1': 77.90196078431373,\n",
       " 'HasAns_total': 30,\n",
       " 'best_exact': 66.66666666666667,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 77.90196078431373,\n",
       " 'best_f1_thresh': 0.0,\n",
       " 'rouge1': 0.7714995189995191,\n",
       " 'rouge2': 0.6562962962962964,\n",
       " 'rougeL': 0.7745815295815295,\n",
       " 'rougeLsum': 0.7731349206349207}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_llm(samples, pred_col='llm_answers_with_context')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, exact, f1 and rouge scores is much better if we provide context to the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enriching Large Language Models (LLMs) with a context drawn from a document corpus is an effective strategy to tailor the general reasoning and linguistic interactions of LLMs to your specific data. Nonetheless, it's crucial to realize that simple query and retrieval might not yield the optimal results! A thorough understanding of the data is essential to fully leverage the benefits of a retrieval-based question-answering methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://python.langchain.com/docs/use_cases/question_answering/semantic-search-over-chat\n",
    "- https://python.langchain.com/docs/guides/evaluation/question_answering\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb\n",
    "- https://www.youtube.com/watch?v=FQnvfR8Dmr0\n",
    "- https://platform.openai.com/docs/guides/gpt-best-practices\n",
    "- https://platform.openai.com/docs/tutorials/web-qa-embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bonus: Real project use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, i want to show how we implemented Question Answering with LLM in an actual customer project. I'll highlight the challenges we encountered and discuss the outcomes we achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client operates an online support platform featuring a vast array of topics, each consisting of a central question and a series of associated discussion messages. This platform, while rich in content, was posing a significant challenge for the client's support team who found themselves dedicating substantial amounts of time to address complex customer inquiries.\n",
    "\n",
    "To streamline this process and make information retrieval more efficient, we suggested the deployment of a Large Language Model (LLM). The intention was to leverage the LLM's capabilities for contextual search and question answering, enabling a quicker resolution of customer queries. This approach promised not only to optimize the support team's workflow but also to enhance the overall user experience on the platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team encountered several significant challenges during this project. First, we needed to ensure accuracy despite not having access to a historical dataset of user queries. Secondly, the data we were working with was unstructured and included various types of content such as private information, SQL, logs, code snippets, and XML. Further complicating matters, the data set also contained unrelated threads which were not relevant to the clients' issues. These factors combined to make this a complex, multifaceted problem that required a sophisticated solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are main highlight of the solution:\n",
    "\n",
    "1. 150k messages (HTML)\n",
    "2. Cleaning data from HTML format, removed sensetive data and unrelated threads\n",
    "3. Applied context filtration to provide only relevant context to the model or dont provide any\n",
    "4. Human in the loop evaluation\n",
    "5. Included references to the original content to enable support team verify the responses\n",
    "6. Small amount of ground truth from experts (~10 samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is how solution looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](resources/sol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](resources/obfuscated.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Result\n",
    "\n",
    "As a result, we achieved notable time savings for the support team, reducing the time spent on addressing complex customer inquiries. The user experience on the platform was significantly enhanced, and the overall productivity of the support team improved, showcasing the efficacy and practicality of our AI-powered solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the integration of a Large Language Model offers substantial benefits, it's essential to understand that merely deploying the LLM API will not instantly solve all challenges or maximize the value derived from its use. There are several limitations and hurdles to be aware of:\n",
    "\n",
    "1. **Data Privacy:** Real-world datasets often contain sensitive information that must be carefully handled to maintain privacy standards. Simply feeding all data to the LLM may violate privacy rules, requiring us to develop methods for identifying and removing or anonymizing such sensitive information.\n",
    "\n",
    "2. **Unstructured Data:** LLMs perform best when the data is well-structured and consistent, which is not always the case with real-world data. Unstructured data, such as free text fields, SQL, logs, code snippets, and XML, require significant pre-processing before they can be effectively utilized by the LLM.\n",
    "\n",
    "3. **Data Gathering and Preparation:** Collecting and preparing the data for use with the LLM is a non-trivial task that can demand significant time and resources. This includes data cleaning, removal of irrelevant threads, and formatting the data in a way that the LLM can interpret.\n",
    "\n",
    "4. **Context Relevance:** Not all data in a given dataset may be relevant to the task at hand. Ensuring that the LLM receives only pertinent context to generate accurate and useful responses is an additional challenge that must be addressed.\n",
    "\n",
    "5. **Evaluation and Verification:** While LLMs can generate responses, verifying the accuracy and appropriateness of these responses is another issue. A human-in-the-loop approach can enhance reliability but also adds another step in the process, which can potentially slow down response times and add to resource demands.\n",
    "\n",
    "In summary, while LLMs provide a powerful tool for tasks such as question answering and contextual search, their successful implementation requires careful planning, data handling, and an understanding of the inherent limitations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
